pip install -r requirement.txt
python -m pip install --upgrade pip
python -m venv venv
venv\Scripts\activate
deactivate


import sys
sys.executable

pip install -r requirements.txt
set SPARK_HOME=C:\MyMain\config\venv1\Lib\site-packages\pyspark

set PYSPARK_PYTHON=C:\MyMain\config\venv1\Scripts\python.exe
set PYSPARK_DRIVER_PYTHON=C:\MyMain\config\venv1\Scripts\python.exe


netstat -a -o
taskkill /f /pid PID번호


protoc -I=protos --python_out=protos protos/delivery_status.proto


protoc --proto_path=. -I . --python_out=. --pyi_out=. ./realtime_status.proto
==================================================================


# 기존 컨테이너 중지 및 제거 (선택 사항)
docker-compose down

# 도커 이미지 빌드
docker-compose build
docker build -f docker/Dockerfile -t my_custom_image_name:1.0 .



# 도커 컨테이너 실행
docker-compose up -d

cd C:\MyMain\test\
docker-compose -f docker/docker-compose.yaml up


# 실행 상태 확인
docker-compose ps

# 로그 실시간 확인 (필요 시)
docker-compose logs -f

docker-compose build --no-cache
docker-compose up

=============================================================
# Kafka CLI 명령어를 사용하여 토픽 목록 조회
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092


- 토픽 확인법
	컨테이너 내부 진입 만약 잘 안된다면 kafka 설치 경로를 찾아야함
	토픽 검색
	/usr/bin/kafka-topics --bootstrap-server localhost:9092 --delete --topic delivery-data

==================================================================

	main/
	├── src/
	│   ├── googleApi.py
	│   ├── producer.py
	│   ├── consumer.py
	│   ├── main.py
	│   ├── config_manager.py
	├── proto/
	│   ├── realtime_status.proto
	│   ├── realtime_status_pb2.py
	│   ├── realtime_status_pb2.pyi
	│   ├── monthly_analysis.proto
	│   ├── monthly_analysis_pb2.py
	│   ├── monthly_analysis_pb2.pyi
	│   ├── weekly_analysis_pb2.py
	│   ├── weekly_analysis_pb2.pyi
	│   ├── weekly_analysis.proto
	├── kafka/
	│   ├── docker-compose.yaml
	│   ├── DockerFile
	├── config/
	│   ├── requirements.txt
	├── oauth/
	│   ├── google/ 
	│   │   ├── credentials.json


1. 실시간 배송 상태 및 KPI 모니터링
목표: 오늘 날짜의 각 배송 상태별 갯수, 완료율, 평균 배송 시간을 모니터링하여 실시간으로 성과를 확인.

2. 이번주 이슈 모니터링
목표: 이번 주 동안 발생한 이슈 갯수와 관련된 DPS 번호를 추적.

3. 이번주 배송 거리 분석
목표: 이번 주의 평균 배송 거리를 계산하여 분석.

4. 이번달 배송 완료율 트렌드 분석
목표: 주별로 배송 완료율의 변화를 분석하여 성과 추적.

5. 이번달 이슈 발생 패턴 분석
목표: 요일별 이슈 발생 패턴을 분석하여 빈도 파악.

6. 이번달 배송 갯수 트렌드 분석
목표 : SLA 타입별, 요일별 배송 개수 분석



==================================================================

commons-pool2-2.11.1.jar
kafka-clients-3.4.0.jar
spark-protobuf_2.12-3.4.1.jar
spark-sql-kafka-0-10_2.12-3.4.1.jar
spark-token-provider-kafka-0-10_2.12-3.4.1.jar




==================================================================

1. **데이터 소스에서 수집**
    - 기술 스택: Kafka, Python, Flink
    - 데이터가 원시 형태로 수집됩니다.

2. **데이터 레이크 저장**
    - 기술 스택: AWS S3, GCS, HDFS
    - 수집된 원시 데이터가 저장됩니다.

3. **데이터 전처리**
    - 기술 스택: Spark, Flink, Python
    - 데이터를 정제하고 변환하여 처리합니다.

4. **데이터 웨어하우스 저장**
    - 기술 스택: AWS Redshift, BigQuery, Snowflake
    - 전처리된 데이터가 구조화된 형태로 저장됩니다.

5. **데이터 마트로 분석 제공**
    - 기술 스택: SQL, OLAP Tools
    - 비즈니스 요구에 맞게 데이터를 추출하여 분석에 활용합니다.

6. **데이터 자동화**
    - 기술 스택: Airflow, Lambda
    - 파이프라인 자동화를 통해 스케줄링 및 트리거 실행을 관리합니다.

7. **클라우드 인프라 활용**
    - 기술 스택: AWS, GCP, Azure
    - 클라우드 기반에서 데이터 저장 및 처리, 자동화가 이루어집니다.
